{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeuye1Z7zsor",
        "outputId": "a18714c9-d81a-4b3f-9123-bcbc83137fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "Xk1KJMM4z2Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "oyzza6syz3Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define data augmentation for training and basic transforms for validation and test\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Paths to data\n",
        "train_extract_path = '/content/drive/MyDrive/DogHeart/train/Dog_heart/Train'\n",
        "validation_extract_path = '/content/drive/MyDrive/DogHeart/train/Dog_heart/Valid'\n",
        "\n",
        "# Load datasets with transformations\n",
        "train_dataset = datasets.ImageFolder(root=train_extract_path, transform=train_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "validation_dataset = datasets.ImageFolder(root=validation_extract_path, transform=val_test_transforms)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the Bottleneck Block\n",
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
        "        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(mid_channels)\n",
        "        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if self.downsample:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the ResNet-Inspired Model\n",
        "class ResNetDogHeart(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(ResNetDogHeart, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Define the ResNet layers\n",
        "        self.layer1 = self._make_layer(64, 64, 256, blocks=3)\n",
        "        self.layer2 = self._make_layer(256, 128, 512, blocks=4, stride=2)\n",
        "        self.layer3 = self._make_layer(512, 256, 1024, blocks=6, stride=2)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, mid_channels, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(BottleneckBlock(in_channels, mid_channels, out_channels, stride, downsample))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BottleneckBlock(out_channels, mid_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, optimizer, and scheduler\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNetDogHeart(num_classes=3).to(device)\n",
        "\n",
        "# Custom weight initialization using Xavier initialization\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        nn.init.constant_(m.weight, 1)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "# Apply custom initialization\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
        "\n",
        "# Define the training function\n",
        "def train_resnet_model(model, train_loader, validation_loader, criterion, optimizer, scheduler=None, num_epochs=20, save_path=\"resnet_model.pt\"):\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in validation_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Accuracy calculation\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(validation_loader)\n",
        "        val_accuracy = 100 * correct / total\n",
        "\n",
        "        # Print training and validation results\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, \"\n",
        "              f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Save model if validation loss improves\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Best model saved with Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "        # Step scheduler\n",
        "        if scheduler:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "    print(\"Training complete. Best model saved.\")\n",
        "\n",
        "# Train the model\n",
        "train_resnet_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, num_epochs=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyd9ROQ6z8TQ",
        "outputId": "53947453-09dd-4c10-93f4-c42b5585f7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Train Loss: 1.4531, Validation Loss: 27.1411, Validation Accuracy: 31.00%\n",
            "Best model saved with Validation Loss: 27.1411\n",
            "Epoch [2/50], Train Loss: 0.9765, Validation Loss: 1.0024, Validation Accuracy: 47.50%\n",
            "Best model saved with Validation Loss: 1.0024\n",
            "Epoch [3/50], Train Loss: 1.0288, Validation Loss: 0.8611, Validation Accuracy: 52.50%\n",
            "Best model saved with Validation Loss: 0.8611\n",
            "Epoch [4/50], Train Loss: 0.9421, Validation Loss: 0.9800, Validation Accuracy: 48.00%\n",
            "Epoch [5/50], Train Loss: 0.8989, Validation Loss: 1.8163, Validation Accuracy: 39.50%\n",
            "Epoch [6/50], Train Loss: 0.8716, Validation Loss: 0.8869, Validation Accuracy: 52.50%\n",
            "Epoch [7/50], Train Loss: 0.8684, Validation Loss: 1.2711, Validation Accuracy: 34.50%\n",
            "Epoch [8/50], Train Loss: 0.8746, Validation Loss: 0.9957, Validation Accuracy: 44.50%\n",
            "Epoch [9/50], Train Loss: 0.8656, Validation Loss: 1.0496, Validation Accuracy: 52.50%\n",
            "Epoch [10/50], Train Loss: 0.8864, Validation Loss: 1.0137, Validation Accuracy: 48.50%\n",
            "Epoch [11/50], Train Loss: 0.8216, Validation Loss: 1.0934, Validation Accuracy: 53.00%\n",
            "Epoch [12/50], Train Loss: 0.8579, Validation Loss: 1.0226, Validation Accuracy: 51.00%\n",
            "Epoch [13/50], Train Loss: 0.8426, Validation Loss: 0.8188, Validation Accuracy: 58.00%\n",
            "Best model saved with Validation Loss: 0.8188\n",
            "Epoch [14/50], Train Loss: 0.8395, Validation Loss: 1.0017, Validation Accuracy: 47.00%\n",
            "Epoch [15/50], Train Loss: 0.8014, Validation Loss: 1.1002, Validation Accuracy: 51.00%\n",
            "Epoch [16/50], Train Loss: 0.8419, Validation Loss: 0.9845, Validation Accuracy: 57.50%\n",
            "Epoch [17/50], Train Loss: 0.8308, Validation Loss: 1.2394, Validation Accuracy: 42.00%\n",
            "Epoch [18/50], Train Loss: 0.8573, Validation Loss: 0.9426, Validation Accuracy: 57.00%\n",
            "Epoch [19/50], Train Loss: 0.8886, Validation Loss: 1.0535, Validation Accuracy: 44.50%\n",
            "Epoch [20/50], Train Loss: 0.8181, Validation Loss: 0.8963, Validation Accuracy: 51.00%\n",
            "Epoch [21/50], Train Loss: 0.8212, Validation Loss: 0.8719, Validation Accuracy: 53.50%\n",
            "Epoch [22/50], Train Loss: 0.8381, Validation Loss: 1.0494, Validation Accuracy: 51.00%\n",
            "Epoch [23/50], Train Loss: 0.7835, Validation Loss: 1.0893, Validation Accuracy: 48.50%\n",
            "Epoch [24/50], Train Loss: 0.8033, Validation Loss: 0.8436, Validation Accuracy: 55.50%\n",
            "Epoch [25/50], Train Loss: 0.7794, Validation Loss: 1.2305, Validation Accuracy: 48.00%\n",
            "Epoch [26/50], Train Loss: 0.8171, Validation Loss: 0.9424, Validation Accuracy: 53.50%\n",
            "Epoch [27/50], Train Loss: 0.8015, Validation Loss: 1.2598, Validation Accuracy: 56.50%\n",
            "Epoch [28/50], Train Loss: 0.7763, Validation Loss: 0.7589, Validation Accuracy: 60.00%\n",
            "Best model saved with Validation Loss: 0.7589\n",
            "Epoch [29/50], Train Loss: 0.7476, Validation Loss: 0.8599, Validation Accuracy: 58.50%\n",
            "Epoch [30/50], Train Loss: 0.7971, Validation Loss: 0.9392, Validation Accuracy: 54.50%\n",
            "Epoch [31/50], Train Loss: 0.7632, Validation Loss: 0.8214, Validation Accuracy: 55.50%\n",
            "Epoch [32/50], Train Loss: 0.7938, Validation Loss: 0.8008, Validation Accuracy: 61.50%\n",
            "Epoch [33/50], Train Loss: 0.7371, Validation Loss: 0.7874, Validation Accuracy: 63.00%\n",
            "Epoch [34/50], Train Loss: 0.7627, Validation Loss: 0.8106, Validation Accuracy: 60.50%\n",
            "Epoch [35/50], Train Loss: 0.7717, Validation Loss: 1.2490, Validation Accuracy: 50.00%\n",
            "Epoch [36/50], Train Loss: 0.7482, Validation Loss: 0.7516, Validation Accuracy: 62.50%\n",
            "Best model saved with Validation Loss: 0.7516\n",
            "Epoch [37/50], Train Loss: 0.7528, Validation Loss: 0.8581, Validation Accuracy: 59.00%\n",
            "Epoch [38/50], Train Loss: 0.7598, Validation Loss: 0.7298, Validation Accuracy: 61.00%\n",
            "Best model saved with Validation Loss: 0.7298\n",
            "Epoch [39/50], Train Loss: 0.7671, Validation Loss: 0.8265, Validation Accuracy: 59.00%\n",
            "Epoch [40/50], Train Loss: 0.7350, Validation Loss: 1.2168, Validation Accuracy: 52.00%\n",
            "Epoch [41/50], Train Loss: 0.7440, Validation Loss: 0.7654, Validation Accuracy: 58.50%\n",
            "Epoch [42/50], Train Loss: 0.7466, Validation Loss: 0.9018, Validation Accuracy: 55.00%\n",
            "Epoch [43/50], Train Loss: 0.7387, Validation Loss: 0.8484, Validation Accuracy: 57.00%\n",
            "Epoch [44/50], Train Loss: 0.7279, Validation Loss: 0.8203, Validation Accuracy: 59.50%\n",
            "Epoch [45/50], Train Loss: 0.7366, Validation Loss: 0.9221, Validation Accuracy: 57.50%\n",
            "Epoch [46/50], Train Loss: 0.7226, Validation Loss: 0.9819, Validation Accuracy: 55.00%\n",
            "Epoch [47/50], Train Loss: 0.7439, Validation Loss: 0.7429, Validation Accuracy: 60.00%\n",
            "Epoch [48/50], Train Loss: 0.7469, Validation Loss: 0.7747, Validation Accuracy: 62.50%\n",
            "Epoch [49/50], Train Loss: 0.7500, Validation Loss: 0.7483, Validation Accuracy: 64.00%\n",
            "Epoch [50/50], Train Loss: 0.7317, Validation Loss: 0.7354, Validation Accuracy: 60.50%\n",
            "Training complete. Best model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define data augmentation for training and basic transforms for validation and test\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Paths to data\n",
        "train_extract_path = '/content/drive/MyDrive/DogHeart/train/Dog_heart/Train'\n",
        "validation_extract_path = '/content/drive/MyDrive/DogHeart/train/Dog_heart/Valid'\n",
        "\n",
        "# Load datasets with transformations\n",
        "train_dataset = datasets.ImageFolder(root=train_extract_path, transform=train_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "validation_dataset = datasets.ImageFolder(root=validation_extract_path, transform=val_test_transforms)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the ResNet-Inspired Model\n",
        "class ResNetDogHeart(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(ResNetDogHeart, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(64, 64, 256, blocks=3)\n",
        "        self.layer2 = self._make_layer(256, 128, 512, blocks=4, stride=2)\n",
        "        self.layer3 = self._make_layer(512, 256, 1024, blocks=6, stride=2)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, mid_channels, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(BottleneckBlock(in_channels, mid_channels, out_channels, stride, downsample))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BottleneckBlock(out_channels, mid_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNetDogHeart(num_classes=3).to(device)\n",
        "\n",
        "# Load the model weights from specified path\n",
        "model_path = '/content/resnet_model.pt'\n",
        "if os.path.exists(model_path):\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    print(\"Model loaded successfully from {}\".format(model_path))\n",
        "else:\n",
        "    print(\"No model found at {}\".format(model_path))\n",
        "    # Initialize weights if no pretrained model\n",
        "    model.apply(initialize_weights)\n",
        "\n",
        "# Define the loss function, optimizer, and learning rate scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
        "\n",
        "# Define the training function\n",
        "def train_resnet_model(model, train_loader, validation_loader, criterion, optimizer, scheduler=None, num_epochs=50, save_path=\"/content/optimized_model.pt\"):\n",
        "    best_val_loss = float(\"inf\")\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in validation_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(validation_loader)\n",
        "        val_accuracy = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Save model if validation loss improves\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Best model saved with Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "        # Step scheduler\n",
        "        if scheduler:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "    print(\"Training complete. Best model saved at {}\".format(save_path))\n",
        "\n",
        "# Train the model for additional 50 epochs\n",
        "train_resnet_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, num_epochs=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZQTEvB483Go",
        "outputId": "da7c73f8-c55d-4852-faa5-caf95f3f3499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from /content/resnet_model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-c32cbe128985>:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Train Loss: 0.8175, Validation Loss: 1.2618, Validation Accuracy: 55.50%\n",
            "Best model saved with Validation Loss: 1.2618\n",
            "Epoch [2/50], Train Loss: 0.7953, Validation Loss: 0.8371, Validation Accuracy: 58.00%\n",
            "Best model saved with Validation Loss: 0.8371\n",
            "Epoch [3/50], Train Loss: 0.7794, Validation Loss: 1.1862, Validation Accuracy: 49.50%\n",
            "Epoch [4/50], Train Loss: 0.7933, Validation Loss: 0.8574, Validation Accuracy: 57.50%\n",
            "Epoch [5/50], Train Loss: 0.7799, Validation Loss: 0.8239, Validation Accuracy: 60.50%\n",
            "Best model saved with Validation Loss: 0.8239\n",
            "Epoch [6/50], Train Loss: 0.7892, Validation Loss: 0.7944, Validation Accuracy: 57.00%\n",
            "Best model saved with Validation Loss: 0.7944\n",
            "Epoch [7/50], Train Loss: 0.7419, Validation Loss: 0.8777, Validation Accuracy: 63.00%\n",
            "Epoch [8/50], Train Loss: 0.7494, Validation Loss: 0.8084, Validation Accuracy: 60.50%\n",
            "Epoch [9/50], Train Loss: 0.7719, Validation Loss: 0.8972, Validation Accuracy: 56.50%\n",
            "Epoch [10/50], Train Loss: 0.7550, Validation Loss: 0.8409, Validation Accuracy: 58.00%\n",
            "Epoch [11/50], Train Loss: 0.7468, Validation Loss: 0.7807, Validation Accuracy: 60.50%\n",
            "Best model saved with Validation Loss: 0.7807\n",
            "Epoch [12/50], Train Loss: 0.7482, Validation Loss: 1.0032, Validation Accuracy: 48.50%\n",
            "Epoch [13/50], Train Loss: 0.7198, Validation Loss: 0.8306, Validation Accuracy: 64.00%\n",
            "Epoch [14/50], Train Loss: 0.7475, Validation Loss: 0.8064, Validation Accuracy: 58.50%\n",
            "Epoch [15/50], Train Loss: 0.7248, Validation Loss: 0.9182, Validation Accuracy: 54.00%\n",
            "Epoch [16/50], Train Loss: 0.7381, Validation Loss: 1.0334, Validation Accuracy: 55.50%\n",
            "Epoch [17/50], Train Loss: 0.7510, Validation Loss: 1.0672, Validation Accuracy: 49.50%\n",
            "Epoch [18/50], Train Loss: 0.7159, Validation Loss: 0.9547, Validation Accuracy: 57.50%\n",
            "Epoch [19/50], Train Loss: 0.7237, Validation Loss: 0.8055, Validation Accuracy: 58.50%\n",
            "Epoch [20/50], Train Loss: 0.7390, Validation Loss: 0.7385, Validation Accuracy: 59.00%\n",
            "Best model saved with Validation Loss: 0.7385\n",
            "Epoch [21/50], Train Loss: 0.7435, Validation Loss: 1.0523, Validation Accuracy: 50.50%\n",
            "Epoch [22/50], Train Loss: 0.7220, Validation Loss: 0.9219, Validation Accuracy: 60.00%\n",
            "Epoch [23/50], Train Loss: 0.7121, Validation Loss: 0.8406, Validation Accuracy: 63.00%\n",
            "Epoch [24/50], Train Loss: 0.7200, Validation Loss: 0.7577, Validation Accuracy: 63.00%\n",
            "Epoch [25/50], Train Loss: 0.7224, Validation Loss: 0.9222, Validation Accuracy: 51.50%\n",
            "Epoch [26/50], Train Loss: 0.7306, Validation Loss: 1.1660, Validation Accuracy: 49.50%\n",
            "Epoch [27/50], Train Loss: 0.7170, Validation Loss: 0.8143, Validation Accuracy: 61.00%\n",
            "Epoch [28/50], Train Loss: 0.6879, Validation Loss: 0.7628, Validation Accuracy: 62.00%\n",
            "Epoch [29/50], Train Loss: 0.7240, Validation Loss: 0.8100, Validation Accuracy: 60.50%\n",
            "Epoch [30/50], Train Loss: 0.6916, Validation Loss: 0.7461, Validation Accuracy: 60.50%\n",
            "Epoch [31/50], Train Loss: 0.7110, Validation Loss: 0.7583, Validation Accuracy: 61.50%\n",
            "Epoch [32/50], Train Loss: 0.7163, Validation Loss: 0.8672, Validation Accuracy: 55.50%\n",
            "Epoch [33/50], Train Loss: 0.7210, Validation Loss: 0.7753, Validation Accuracy: 60.50%\n",
            "Epoch [34/50], Train Loss: 0.7101, Validation Loss: 0.7347, Validation Accuracy: 60.00%\n",
            "Best model saved with Validation Loss: 0.7347\n",
            "Epoch [35/50], Train Loss: 0.7099, Validation Loss: 0.7028, Validation Accuracy: 67.00%\n",
            "Best model saved with Validation Loss: 0.7028\n",
            "Epoch [36/50], Train Loss: 0.6980, Validation Loss: 1.1528, Validation Accuracy: 49.00%\n",
            "Epoch [37/50], Train Loss: 0.7080, Validation Loss: 0.8656, Validation Accuracy: 57.50%\n",
            "Epoch [38/50], Train Loss: 0.7125, Validation Loss: 0.7331, Validation Accuracy: 62.50%\n",
            "Epoch [39/50], Train Loss: 0.6952, Validation Loss: 0.7067, Validation Accuracy: 68.00%\n",
            "Epoch [40/50], Train Loss: 0.6931, Validation Loss: 0.7144, Validation Accuracy: 65.50%\n",
            "Epoch [41/50], Train Loss: 0.6694, Validation Loss: 0.7967, Validation Accuracy: 57.50%\n",
            "Epoch [42/50], Train Loss: 0.6817, Validation Loss: 0.7105, Validation Accuracy: 65.00%\n",
            "Epoch [43/50], Train Loss: 0.7221, Validation Loss: 0.7753, Validation Accuracy: 58.00%\n",
            "Epoch [44/50], Train Loss: 0.6796, Validation Loss: 0.7463, Validation Accuracy: 67.00%\n",
            "Epoch [45/50], Train Loss: 0.6927, Validation Loss: 0.7711, Validation Accuracy: 67.00%\n",
            "Epoch [46/50], Train Loss: 0.6761, Validation Loss: 0.7575, Validation Accuracy: 63.50%\n",
            "Epoch [47/50], Train Loss: 0.6663, Validation Loss: 0.9385, Validation Accuracy: 54.00%\n",
            "Epoch [48/50], Train Loss: 0.7008, Validation Loss: 0.8168, Validation Accuracy: 56.50%\n",
            "Epoch [49/50], Train Loss: 0.6743, Validation Loss: 0.9452, Validation Accuracy: 53.50%\n",
            "Epoch [50/50], Train Loss: 0.6958, Validation Loss: 0.8172, Validation Accuracy: 60.00%\n",
            "Training complete. Best model saved at /content/optimized_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming the model class ResNetDogHeart and the initialization function are already defined as per your previous setup\n",
        "\n",
        "# Path to the pre-trained model\n",
        "model_path = '/content/optimized_model.pt'\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the model\n",
        "model = ResNetDogHeart(num_classes=3).to(device)\n",
        "\n",
        "# Load the model weights\n",
        "if os.path.exists(model_path):\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    print(\"Loaded model weights from:\", model_path)\n",
        "else:\n",
        "    print(\"Model file not found, check the path and try again.\")\n",
        "    # Optionally initialize or exit\n",
        "    # model.apply(initialize_weights) or exit()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, num_epochs, save_path):\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in validation_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(validation_loader)\n",
        "        val_accuracy = 100 * correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Saved better model to {save_path}\")\n",
        "\n",
        "        # Update the learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "# Set path for saving the model\n",
        "save_model_path = '/content/optimized_model_updated.pt'\n",
        "\n",
        "# Call the training function\n",
        "train_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, 50, save_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3RhA_4aEyyD",
        "outputId": "ae76395b-185c-42f0-bcd9-c9694a179fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model weights from: /content/optimized_model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-9b0124556217>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 0.7114, Val Loss: 0.8563, Val Accuracy: 57.50%\n",
            "Saved better model to /content/optimized_model_updated.pt\n",
            "Epoch 2/50, Train Loss: 0.7251, Val Loss: 0.8008, Val Accuracy: 56.00%\n",
            "Saved better model to /content/optimized_model_updated.pt\n",
            "Epoch 3/50, Train Loss: 0.7014, Val Loss: 0.7585, Val Accuracy: 60.00%\n",
            "Saved better model to /content/optimized_model_updated.pt\n",
            "Epoch 4/50, Train Loss: 0.7177, Val Loss: 0.7439, Val Accuracy: 60.00%\n",
            "Saved better model to /content/optimized_model_updated.pt\n",
            "Epoch 5/50, Train Loss: 0.6978, Val Loss: 0.7143, Val Accuracy: 65.50%\n",
            "Saved better model to /content/optimized_model_updated.pt\n",
            "Epoch 6/50, Train Loss: 0.6833, Val Loss: 0.7112, Val Accuracy: 64.00%\n",
            "Saved better model to /content/optimized_model_updated.pt\n",
            "Epoch 7/50, Train Loss: 0.6797, Val Loss: 0.6709, Val Accuracy: 66.50%\n",
            "Saved better model to /content/optimized_model_updated.pt\n",
            "Epoch 8/50, Train Loss: 0.6800, Val Loss: 0.9642, Val Accuracy: 51.00%\n",
            "Epoch 9/50, Train Loss: 0.7025, Val Loss: 1.0008, Val Accuracy: 52.50%\n",
            "Epoch 10/50, Train Loss: 0.6740, Val Loss: 0.6781, Val Accuracy: 68.50%\n",
            "Epoch 11/50, Train Loss: 0.6612, Val Loss: 0.7778, Val Accuracy: 58.50%\n",
            "Epoch 12/50, Train Loss: 0.6613, Val Loss: 0.7041, Val Accuracy: 66.50%\n",
            "Epoch 13/50, Train Loss: 0.6694, Val Loss: 0.6747, Val Accuracy: 68.00%\n",
            "Epoch 14/50, Train Loss: 0.6736, Val Loss: 0.6636, Val Accuracy: 67.00%\n",
            "Saved better model to /content/optimized_model_updated.pt\n",
            "Epoch 15/50, Train Loss: 0.6448, Val Loss: 0.6799, Val Accuracy: 67.00%\n",
            "Epoch 16/50, Train Loss: 0.6598, Val Loss: 0.6548, Val Accuracy: 69.00%\n",
            "Saved better model to /content/optimized_model_updated.pt\n",
            "Epoch 17/50, Train Loss: 0.6464, Val Loss: 0.6877, Val Accuracy: 67.00%\n",
            "Epoch 18/50, Train Loss: 0.6335, Val Loss: 0.6614, Val Accuracy: 66.50%\n",
            "Epoch 19/50, Train Loss: 0.6470, Val Loss: 0.6813, Val Accuracy: 68.00%\n",
            "Epoch 20/50, Train Loss: 0.6389, Val Loss: 0.7067, Val Accuracy: 65.50%\n",
            "Epoch 21/50, Train Loss: 0.6158, Val Loss: 0.7004, Val Accuracy: 66.00%\n",
            "Epoch 22/50, Train Loss: 0.6231, Val Loss: 0.6711, Val Accuracy: 66.50%\n",
            "Epoch 23/50, Train Loss: 0.6196, Val Loss: 0.6729, Val Accuracy: 68.00%\n",
            "Epoch 24/50, Train Loss: 0.6074, Val Loss: 0.7292, Val Accuracy: 68.00%\n",
            "Epoch 25/50, Train Loss: 0.6223, Val Loss: 0.6687, Val Accuracy: 67.50%\n",
            "Epoch 26/50, Train Loss: 0.5936, Val Loss: 0.6793, Val Accuracy: 70.00%\n",
            "Epoch 27/50, Train Loss: 0.6001, Val Loss: 0.6597, Val Accuracy: 67.00%\n",
            "Epoch 28/50, Train Loss: 0.6177, Val Loss: 0.7547, Val Accuracy: 65.50%\n",
            "Epoch 29/50, Train Loss: 0.5971, Val Loss: 0.7121, Val Accuracy: 68.00%\n",
            "Epoch 30/50, Train Loss: 0.5933, Val Loss: 0.6841, Val Accuracy: 67.50%\n",
            "Epoch 31/50, Train Loss: 0.5989, Val Loss: 0.7181, Val Accuracy: 68.00%\n",
            "Epoch 32/50, Train Loss: 0.6035, Val Loss: 0.6855, Val Accuracy: 70.00%\n",
            "Epoch 33/50, Train Loss: 0.5965, Val Loss: 0.6883, Val Accuracy: 70.50%\n",
            "Epoch 34/50, Train Loss: 0.5939, Val Loss: 0.7148, Val Accuracy: 70.00%\n",
            "Epoch 35/50, Train Loss: 0.5731, Val Loss: 0.7177, Val Accuracy: 70.00%\n",
            "Epoch 36/50, Train Loss: 0.6011, Val Loss: 0.7079, Val Accuracy: 70.00%\n",
            "Epoch 37/50, Train Loss: 0.5924, Val Loss: 0.6929, Val Accuracy: 74.00%\n",
            "Epoch 38/50, Train Loss: 0.5855, Val Loss: 0.7093, Val Accuracy: 70.00%\n",
            "Epoch 39/50, Train Loss: 0.5745, Val Loss: 0.7035, Val Accuracy: 69.50%\n",
            "Epoch 40/50, Train Loss: 0.5852, Val Loss: 0.6844, Val Accuracy: 70.00%\n",
            "Epoch 41/50, Train Loss: 0.5725, Val Loss: 0.6993, Val Accuracy: 69.00%\n",
            "Epoch 42/50, Train Loss: 0.5732, Val Loss: 0.7131, Val Accuracy: 71.00%\n",
            "Epoch 43/50, Train Loss: 0.5685, Val Loss: 0.6906, Val Accuracy: 71.00%\n",
            "Epoch 44/50, Train Loss: 0.5744, Val Loss: 0.7044, Val Accuracy: 69.50%\n",
            "Epoch 45/50, Train Loss: 0.5677, Val Loss: 0.7223, Val Accuracy: 70.00%\n",
            "Epoch 46/50, Train Loss: 0.5759, Val Loss: 0.7007, Val Accuracy: 69.00%\n",
            "Epoch 47/50, Train Loss: 0.5627, Val Loss: 0.6956, Val Accuracy: 69.00%\n",
            "Epoch 48/50, Train Loss: 0.5730, Val Loss: 0.7208, Val Accuracy: 69.00%\n",
            "Epoch 49/50, Train Loss: 0.5715, Val Loss: 0.7030, Val Accuracy: 68.50%\n",
            "Epoch 50/50, Train Loss: 0.5595, Val Loss: 0.7077, Val Accuracy: 72.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import csv\n",
        "\n",
        "def test_model(model, test_loader, output_file='test_results.csv'):\n",
        "    model.eval()\n",
        "    results = []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, filenames in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            results.extend(zip(filenames, preds.cpu().numpy()))\n",
        "\n",
        "    if results:\n",
        "        with open(output_file, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerows(results)\n",
        "        print(f\"Test results saved to {output_file}\")\n",
        "    else:\n",
        "        print(\"No predictions were generated. Check if your test_loader has data.\")\n",
        "\n",
        "test_model(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsRJx1DcGDeo",
        "outputId": "2ed36461-c160-4cab-db56-6c59c9f0bf6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test results saved to test_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![69.5.JPG](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "zBsUUzFrMPp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "mTK34b3_DoOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://drive.google.com/file/d/1bHXGTe3xTusA-sLx2OlGcA9SLEESqqQi/view"
      ],
      "metadata": {
        "id": "IQR6BLYaMPPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Paper"
      ],
      "metadata": {
        "id": "2F2QbbqaDrIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://www.researchgate.net/publication/385982564_Custom_Residual_CNN_for_Multi-Class_Image_Classification_on_Dog_Heart_Data"
      ],
      "metadata": {
        "id": "FQO-4XyH9-z0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}